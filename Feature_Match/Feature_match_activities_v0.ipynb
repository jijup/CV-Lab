{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**PROJECT 2: FEATURE MATCHING**\n",
        "\n",
        "##**1. Background**\n",
        "In this project, first you will familiarize yourself with OpenCV functions for computing image derivatives and gradients. In particular, you will use the Sobel operators to find edge features in an input image. Then you will write functions for feature matching.\n",
        "\n",
        "Feature matching is the process of recognizing features of the same object(s) across images of a scene. In this project, we will learn how to extract distinctive key points from an image, how to compute a local feature descriptor from a normalized region around each key point and how to find the corresponding local descriptors of two images of a scene taken from slightly different view points.\n",
        "\n",
        "You have been given two pairs of images, i.e., NotreDame, and Mount_Rushmore. These images are available under the 'data' folder. You may plot these images and observe that each pair of images corresponds to an object, but taken from different view points.  \n",
        "\n",
        "### **Your Tasks:**\n",
        "\n",
        "1.  **Feature/Keypoint Detection:**   Finds interest points in the input images (graded activity). Implement the Harris corner detector (Szeleski 7.1.1).\n",
        "2.   **Feature Description:** Each region around detected keypoint locations is converted into a more compact and stable (invariant) descriptor that can be matched against other descriptors (graded activity). We will use Scale Invariant Feature Transform (SIFT)-like descriptor (Szeleski 7.1.2).\n",
        "3.   **Feature Matching:** Finds matching features in multiple images(graded activity). Implement ratio-test (nearest neighbor distance ratio) method for matching features (Szeleski 7.1.3, Equation 7.18)\n",
        "4.   Finally visualizes the matches\n",
        "\n",
        "Note that we will use some functions from `skimage` library (https://scikit-image.org/) along with `OpenCV` functions. scikit-image (`skimage`) is a collection of algorithms for image processing written in Python."
      ],
      "metadata": {
        "id": "rSQm2IcA2NRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before executing the following python code, you have to upload the `Feature_Matching` folder to your Google drive. Download the project from github repo and upload it into the `My Drive/Colab Notebooks` of your google drive. Now, let us mount the google drive and set the working directory to `My Drive/Colab Notebooks/Fearure_Matching`."
      ],
      "metadata": {
        "id": "QWLQQPtoc4z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP7GTM1-99sn",
        "outputId": "a9e970c4-14b6-47b3-9dc9-09ccd8ba3f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"/content/gdrive/MyDrive/\"\n",
        "project_folder = \"Colab Notebooks/Feature_Match/\"\n",
        "os.chdir(root_dir+project_folder)"
      ],
      "metadata": {
        "id": "xpnhh-ksBH8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Gradient Operators and Canny Edge detector(20%)**\n",
        "\n",
        "Before you start with the feature matching, we ask you to familiarize yourself with the Sobel operators that we discussed in the lecture. OpenCV provides function [Sobel()](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d) to calculate the derivatives from an image. The syntax follows.\n",
        "\n",
        "`dst = cv.Sobel(src, ddepth, dx, dy, ksize, scale, delta, borderType)`\n",
        "\n",
        "Parameters:\n",
        "\n",
        "\n",
        "\n",
        "*  **src**\tinput image.\n",
        "*   **dst**\toutput image of the same size and the same number of channels as src .\n",
        "* **ddepth**\toutput image depth, see [combinations](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#filter_depths); in the case of 8-bit input images it will result in truncated derivatives.\n",
        "* **dx**\torder of the derivative x (vertical edges).\n",
        "*  **dy**  \torder of the derivative y (hrizontal edges).\n",
        "* **ksize**\tsize of the extended Sobel kernel; it must be 1, 3, 5, or 7.\n",
        "* **scale**\t[optional] scale factor for the computed derivative values; by default, no scaling is applied (see getDerivKernels for details).\n",
        "* **delta**\t[optional] delta value that is added to the results prior to storing them in dst.\n",
        "* **borderType**\tpixel extrapolation method, see BorderTypes. BORDER_WRAP is not supported.\n",
        "\n",
        "\n",
        "In the following code, we load the `lenna` image, convert it to a gray scale image for ease of computation (need to handle only one channel), and smooths that image using the Gaussian kernel. Then we apply the sobel() function on the `lenna` image to get the vertical and horizontal derivatives. Further, we use a weighted combination of these derivatives values as the final image gradients. \n",
        "\n",
        "Your task is to compute the gradient magnitude and oprientations and display the corresponding images. To compute the actual image gradients and orientations, you can use the gradiant magnitude formula from the lecture slide. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qyKMQ8AJeMUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Graded activity 1\n",
        "\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "from skimage import io, filters, feature\n",
        "\n",
        "#Read the image\n",
        "lenna = cv.imread(\"./data/lenna.bmp\", 1)\n",
        "\n",
        "#Convert it to a gray scale aimge\n",
        "gray = cv.cvtColor(lenna, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "#Remove noise\n",
        "gray_smooth = cv.GaussianBlur(gray, (3, 3), 0)\n",
        "\n",
        "#Sobel Operator (horizontal)\n",
        "grad_x = cv.Sobel(gray_smooth, cv.CV_32F, 1, 0, ksize=3, borderType=cv.BORDER_DEFAULT)\n",
        "\n",
        "#Sobel Operator (vertical)\n",
        "grad_y = cv.Sobel(gray_smooth, cv.CV_32F, 0, 1, ksize=3, borderType=cv.BORDER_DEFAULT)\n",
        "\n",
        "#Convert to uint8\n",
        "abs_grad_x = cv.convertScaleAbs(grad_x)\n",
        "abs_grad_y = cv.convertScaleAbs(grad_y)    \n",
        "    \n",
        "grad = cv.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0)\n",
        "\n",
        "cv2_imshow(grad)\n",
        "\n",
        "# Compute magnitude and orientations\n",
        "\n",
        "#compute the squares, cv.multiply()\n",
        "sobelx2 = _________\n",
        "sobely2 = _________\n",
        "\n",
        "#compute the square root of the sum, check cv.sqrt()\n",
        "magnitude = _________\n",
        "\n",
        "cv2_imshow(magnitude)\n",
        "\n",
        "\n",
        "#compute the orientation, you may use cv.phase()\n",
        "orientation = __________\n",
        "\n",
        "orientation = orientation / 2. \n",
        "hsv = np.zeros_like(lenna)\n",
        "hsv[..., 0] = orientation # H (in OpenCV between 0:180)\n",
        "hsv[..., 1] = 255 # S\n",
        "hsv[..., 2] = cv.normalize(magnitude, None, 0, 255, cv.NORM_MINMAX) # V 0:255\n",
        "\n",
        "\n",
        "bgr = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n",
        "cv2_imshow(bgr)\n",
        "\n"
      ],
      "metadata": {
        "id": "YSw_sRsce5er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next task for you is to try different gradient operators available in OpenCV. Try Prewitt, Roberts and Canny edge detectors on `smu` image. Ry varying the hysteresis thresholds of Canny detector and analyze the results."
      ],
      "metadata": {
        "id": "sKG3IGY_T510"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Graded activity 2\n",
        "smu = cv.imread(\"./data/smu.jpg\", 1)\n",
        "gray= cv.cvtColor(smu, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "smu_gray = cv.GaussianBlur(gray, (3, 3), 0)\n",
        "\n",
        "#Roberts kernel, refer to slides for the v and h kernels\n",
        "roberts_v = np.array(_______ )  \n",
        "roberts_h = np.array(_______ )\n",
        "\n",
        "#Convolve with Roberts kernels\n",
        "r_grad_x = cv.filter2D(smu_gray, cv.CV_32F, roberts_v)\n",
        "r_grad_y = cv.filter2D(smu_gray, cv.CV_32F, roberts_h)\n",
        "\n",
        "robertsx2 = ________\n",
        "robertsy2 = ________\n",
        "r_magnitude = _______\n",
        "\n",
        "cv2_imshow(r_magnitude)\n",
        "\n",
        "#Prewitt kernels, see the lecture slides\n",
        "p_kernelx = np.array(______)\n",
        "p_kernely = np.array(______)\n",
        "\n",
        "\n",
        "#Convolve with Prewitt kernels\n",
        "p_grad_x = cv.filter2D(smu_gray, cv.CV_32F, p_kernelx)\n",
        "p_grad_y = cv.filter2D(smu_gray, cv.CV_32F, p_kernely)\n",
        "\n",
        "prewittx2 = _______\n",
        "prewitty2 = _______\n",
        "\n",
        "p_magnitude = _______\n",
        "\n",
        "cv2_imshow(p_magnitude)\n",
        "\n",
        "\n",
        "#Canny edge detector, see cv.Canny()\n",
        "smu_canny = cv.Canny(smu, 80, 150)\n",
        "cv2_imshow(smu_canny)\n",
        "\n"
      ],
      "metadata": {
        "id": "I31siejXUkha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Load and Display images**"
      ],
      "metadata": {
        "id": "T_XLrSaIPNl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test data is available under the `data` folder. Read a pair of test images. Below I have used the images of famous `NotreDame`, Paris.\n",
        "\n",
        "Load and plot the input images.  When you plot the images, you can see that the images are of NotreDame taken from slightly different view points. Your final objective is to map the corresponding features in both the images. For instance, top left part of the buildin in image 1 should be mapped to the same part of the building in image 2."
      ],
      "metadata": {
        "id": "gJsDRx8NBuJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"NotreDame\"\n",
        "file1 = \"./data/NotreDame/\"+file_name+\"1.jpg\"\n",
        "file2 = \"./data/NotreDame/\"+file_name+\"2.jpg\"\n",
        "\n",
        "#Read color image\n",
        "image1 = cv.imread(file1, 1)\n",
        "image2 = cv.imread(file2, 1)\n",
        "\n",
        "\n",
        "#Plot the images\n",
        "cv2_imshow(image1)\n",
        "cv2_imshow(image2)"
      ],
      "metadata": {
        "id": "wQnIz-LEBw42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "img1 = cv.cvtColor(image1, cv.COLOR_BGR2GRAY)\n",
        "img2 = cv.cvtColor(image2, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "#Resacle the images to their half sizes (bilinear interpolation)[To speed up the computation]\n",
        "\n",
        "img1 = cv.resize(img1, None, fx=0.5, fy=0.5, interpolation = cv.INTER_LINEAR)\n",
        "img2 = cv.resize(img2, None, fx=0.5, fy=0.5, interpolation = cv.INTER_LINEAR)\n",
        "\n",
        "cv2_imshow(img1)\n",
        "cv2_imshow(img2)"
      ],
      "metadata": {
        "id": "RxkKFelnPY6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. Key Points Extraction (30%)**\n",
        "The next task is to extract distinctive key points (or interest points). There are many different ways to find key points in an image. We can use Harris corners detection (to be discussed in the next lecture), largely due to its simplicity. \n",
        "\n",
        "The Harris corners detection algorithm is based on a somewhat intuitive fact: image intensities adjacent to an object corner are generally dissimilar to the intensities at the corner. A method to find corners is to compute image gradients and determine the image locations with large changes in all the directions. The greater the gradient, the more likely a particular point corresponds to an object corner.\n",
        "\n",
        "To the right are the (magnified) results of my own Harris corners detector, run on the left image of the Notre Dame cathedral shown above. The detector tends to fire at corners and edges of the building, and in the center, at the paned round glass window.\n",
        "\n",
        "We first make images smaller to speed up the algorithm. This parameter gets passed into the evaluation code, so don't resize the images except for changing this parameter - We will evaluate your code using scale_factor = 0.5, so be aware of this"
      ],
      "metadata": {
        "id": "knjCldsYEALp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the pixel width and pixel height of each local feature"
      ],
      "metadata": {
        "id": "PR-mM3S5GELo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_width = 16\n",
        "   "
      ],
      "metadata": {
        "id": "dR5NpBEIFjnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Find distinctive points in each image. See Szeliski 4.1.1.\n",
        " You have to implement extract_key_points () function. \n",
        "\n",
        " Implement the Harris corner detector (See Szeliski 4.1.1) to start with.     You do not need to worry about scale invariance or keypoint orientation stimation for your Harris corner detector. If you're finding spurious interest point detections near the boundaries, it is safe to simply suppress the gradients / corners near the edges of  the image. \n",
        " \n",
        " **Useful functions:** A working solution does not require the use of all of these     functions, but depending on your implementation, you may find some useful. Please  reference the documentation for each function/library and feel free to come to office hours\n",
        "   \n",
        "        - skimage.feature.peak_local_max (experiment with different min_distance values to get good results)\n",
        "        - skimage.measure.regionprops\n",
        "\n",
        "  \n",
        " The following is a graded function."
      ],
      "metadata": {
        "id": "BRcnvnbyKsll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Graded Activity 3\n",
        "def extract_key_points(image, feature_width):\n",
        "    '''\n",
        "    Returns key points for the input image\n",
        "\n",
        "    :params:\n",
        "    :image: a grayscale \n",
        "    :feature_width:\n",
        "\n",
        "    :returns:\n",
        "    :xs: an np array of the x coordinates of the interest points in the image\n",
        "    :ys: an np array of the y coordinates of the interest points in the image \n",
        "   \n",
        "    '''\n",
        "    alpha = 0.06\n",
        "    threshold = 0.01\n",
        "    stride = 2\n",
        "    sigma = 0.1\n",
        "    min_distance = 3   \n",
        "\n",
        "    \n",
        "    #step1: blur image. Use filters.gaussian with sigma=0.1, refer to https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.gaussian\n",
        "    filtered_image = filters.gaussian(image, sigma=0.1)\n",
        "    \n",
        "    # step2: calculate gradient of image, I_x and I_y. Use filters.sobel_v and filters.sobel_h operators\n",
        "    I_x = _____________\n",
        "    I_y = _____________\n",
        "\n",
        "    # step3: calculate I_xx, I_xy, I_yy. Useful functions: np.square() and np.multiply(). Refer to https://numpy.org/doc/stable/reference/routines.math.html\n",
        "    I_xx = ____________\n",
        "    I_xy = ____________\n",
        "    I_yy = ____________\n",
        "\n",
        "    I_xx = filters.gaussian(I_xx, sigma=0.1)\n",
        "    I_xy = filters.gaussian(I_xy, sigma=0.1)\n",
        "    I_yy = filters.gaussian(I_yy, sigma=0.1)\n",
        "\n",
        "    listC = np.zeros_like(image)\n",
        "\n",
        "    # step4: caculate C matrix\n",
        "    for y in range(0, image.shape[0]-feature_width, stride):\n",
        "        for x in range(0, image.shape[1]-feature_width, stride):\n",
        "          \n",
        "            # Matrix\n",
        "            Sxx = np.sum(I_xx[y:y+feature_width+1, x:x+feature_width+1])\n",
        "            Syy = np.sum(I_yy[y:y+feature_width+1, x:x+feature_width+1])\n",
        "            Sxy = np.sum(I_xy[y:y+feature_width+1, x:x+feature_width+1])\n",
        "\n",
        "            #COmpute the determinant of C\n",
        "            detC = ______________\n",
        "\n",
        "            #Compute the trace of C\n",
        "            traceC = ______________\n",
        "\n",
        "            C = detC - alpha*(traceC**2)\n",
        "            \n",
        "            if C > threshold:\n",
        "                listC[y+feature_width//2, x+feature_width//2] = C\n",
        "\n",
        "    # step5: using non-maximal suppression\n",
        "    ret = feature.peak_local_max(listC, min_distance=min_distance, threshold_abs=threshold)\n",
        "    return ret[:, 1], ret[:, 0]"
      ],
      "metadata": {
        "id": "PUmtcHX1ROxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us use the function to extract the key points"
      ],
      "metadata": {
        "id": "7WcwLQq9SUEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Key Points Extraction...\")\n",
        "(x1, y1) = extract_key_points(img1, feature_width)\n",
        "(x2, y2) = extract_key_points(img2, feature_width)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "H6ETwD9QGCFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the extracted key points. Use `plt.imshow()` to plot the images. Refer to https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html for the parameter details of `plt.imshow()`.\n",
        "  "
      ],
      "metadata": {
        "id": "juqpujP0SNt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(img1, cmap=\"gray\")\n",
        "plt.scatter(x1, y1, alpha=1, s=2)\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(img2, cmap=\"gray\")\n",
        "plt.scatter(x2, y2, alpha=1, s=2)\n",
        "plt.show()\n",
        "\n",
        "print(\"Key Points Extraction Completed!\")"
      ],
      "metadata": {
        "id": "i1TG3orkLPB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Computing the Feature Descriptor (SIFT)(40%)**\n",
        "Description will be available next week"
      ],
      "metadata": {
        "id": "w6s1ViTLScdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Feature Matching (10%)**\n",
        "\n",
        " Dsecription will be available next week"
      ],
      "metadata": {
        "id": "2VJj1Ya8FRZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. Evaluation Details**\n",
        "This tutorial project is meant to be done in three recitations (Sept 28, Oct. 5 and Oct 12). You will get a total of 16 days to complete the activities. Attendance in each recitation is mandatory to get marks for the graded activities. We will record your attendance. If you complete all the activities in the first recitation itself, then the second recitation is optional. You have to read the instructions, refer to the slides and do all the exercises, refer to the materials as and when necessary (especially for syntaxes). Evaluation will be done in the recitations, mainly in the second recitation. The marker will come evaluate your work during the recitation on Oct. 12. During the evaluation, you will be asked to show the graded activities and you may expect a couple of related questions.\n",
        "\n",
        "**NOTE** In case you are unable to attend the recitation, you have to inform the instructor before the recitation and absence due to sickness orr other genuine grounds will be considered."
      ],
      "metadata": {
        "id": "66RffS38ezWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7. References/Source**\n",
        "1. James Hayes, Georgia Tech, CS 4476-B / 6476-A Computer Vision Course"
      ],
      "metadata": {
        "id": "9zC8kfbwf0u5"
      }
    }
  ]
}